{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bird_Species_Prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14OXnwYeyZSS"
      },
      "source": [
        "# Bird Species Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4wh8OIayhkn"
      },
      "source": [
        "In this project, I created a convolutional Neural Network which will be able to predict species of the bird. I used different layers and other hyperparameters for building, training and testing this multiclass classifictaion model. I used keras for this project."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UlFUS5fyNt2"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRGInah67Hjl"
      },
      "source": [
        "First I mounted my google drive on colab so that I can use the dataset directly from our drive. For this I first uploaded the data on your drive and then mounted the drive on colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wu6mxNRIyQ3M"
      },
      "source": [
        "# After executing the cell above, Drive files will be present in \"/content/drive/My Drive\".\n",
        "!ls \"/content/drive/My Drive\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIZ61VM97OT-"
      },
      "source": [
        "After mounting my drive I will locate the folder where my data is stored to use it in our colab notebook. Here I can see that all the folders I have in my drive and 'Bird Species Dataset' contains the images that I will work on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mJaMUYDyxoD"
      },
      "source": [
        "# Library imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import cv2\n",
        "import random\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import  LabelBinarizer\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwjsH1qm7egp"
      },
      "source": [
        "I started by importing some required libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duP0-XvFy1C9"
      },
      "source": [
        "# Plotting 12 images to check dataset\n",
        "plt.figure(figsize=(12,12))\n",
        "path = \"/content/drive/My Drive/Bird Speciees Dataset/AMERICAN GOLDFINCH\"\n",
        "for i in range(1,17):\n",
        "    plt.subplot(4,4,i)\n",
        "    plt.tight_layout()\n",
        "    rand_img = imread(path +'/'+ random.choice(sorted(listdir(path))))\n",
        "    plt.imshow(rand_img)\n",
        "    plt.xlabel(rand_img.shape[1], fontsize = 10)\n",
        "    plt.ylabel(rand_img.shape[0], fontsize = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxtlrISk7piy"
      },
      "source": [
        "Let's visualize some of the bird images that I will be working on. Also I will observe x and y dimensions of the image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D8FRY23zH2t"
      },
      "source": [
        "# Setting path and creating empty list\n",
        "dir = \"/content/drive/My Drive/Bird Speciees Dataset\"\n",
        "root_dir = listdir(dir)\n",
        "image_list, label_list = [], []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPjqjMI573DX"
      },
      "source": [
        "Setting the root directory for the dataset and storing all the floders name of the dataset. We will also create 2 empty list for image and lables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qY4LwMBzvpx"
      },
      "source": [
        "# Reading and converting image to numpy array\n",
        "for directory in root_dir:\n",
        "  for files in listdir(f\"{dir}/{directory}\"):\n",
        "    image_path = f\"{dir}/{directory}/{files}\"\n",
        "    image = cv2.imread(image_path)\n",
        "    image = img_to_array(image)\n",
        "    image_list.append(image)\n",
        "    label_list.append(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxIjQIrN8RIM"
      },
      "source": [
        "Next we will read all the images and convert it into array and appending the list created above with the image and its label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xg0vsmR0Mb2"
      },
      "source": [
        "# Visualize the number of classes count\n",
        "label_counts = pd.DataFrame(label_list).value_counts()\n",
        "label_counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uUXeHnO8rk7"
      },
      "source": [
        "Check for class imbalance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnZwm5tI4187"
      },
      "source": [
        "# Storing number of classes\n",
        "num_classes = len(label_counts)\n",
        "num_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dVFPt7x8zPc"
      },
      "source": [
        "Next I found out the number of classes that I will have to work upon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI7cyZNF2Ieq"
      },
      "source": [
        "# Checking input image shape\n",
        "image_list[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-Z8eOEz872B"
      },
      "source": [
        "Checking the size of the single image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKE7qOJs2MQm"
      },
      "source": [
        "# Checking labels shape \n",
        "label_list = np.array(label_list)\n",
        "label_list.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tbAEgM99CeW"
      },
      "source": [
        "Checking the shape of image labels which will be equal to the number of images I am going to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtG0rEls2OuD"
      },
      "source": [
        "# Splitting dataset\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state = 10) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJAoH6yH9W2m"
      },
      "source": [
        "Now we will split the data into training and testing using train_test_split() of sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbuIoPIq2Rvn"
      },
      "source": [
        "# Normalize and reshape data\n",
        "x_train = np.array(x_train, dtype=np.float16) / 225.0\n",
        "x_test = np.array(x_test, dtype=np.float16) / 225.0\n",
        "x_train = x_train.reshape( -1, 224,224,3)\n",
        "x_test = x_test.reshape( -1, 224,224,3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab7k7sU59k0q"
      },
      "source": [
        "Next I normalized the images by dividing them with 255. Also, I reshaped x_train and x_test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWjfQATN3Yqh"
      },
      "source": [
        "# Label binarizing\n",
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)\n",
        "print(lb.classes_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtorEYk5-Lls"
      },
      "source": [
        "Next I used label binarizer to one hot encode my data. I also printed the sequence of the classes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQ6wz8Gq4qll"
      },
      "source": [
        "# Splitting the training data set into training and validation data sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLWZvaWj-wUA"
      },
      "source": [
        "Now we will split the training data to validation and training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hEQHzFa3bZk"
      },
      "source": [
        "# Building model architecture\n",
        "model = Sequential()\n",
        "model.add(Conv2D(8, (3, 3), padding=\"same\",input_shape=(224,224,3), activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(3, 3)))\n",
        "model.add(Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7yL1L0x_MST"
      },
      "source": [
        "Next I created a network architecture for the model. I have used different types of layers according to their features namely Conv_2d (It is used to create a convolutional kernel that is convolved with the input layer to produce the output tensor), max_pooling2d (It is a downsampling technique which takes out the maximum value over the window defined by poolsize), flatten (It flattens the input and creates a 1D output), Dense (Dense layer produce the output as the dot product of input and kernel). In the last layer I used softmax as the activation function because it is a multi class classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyI9PKGi4nVf"
      },
      "source": [
        "# Compiling model\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0005),metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noaauIqK_aJk"
      },
      "source": [
        "While compiling the model I need to set the type of loss which will be Categorical Crossentropy for my model alongwith this we also need to set the optimizer and the metrics respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZX1byXc4s2H"
      },
      "source": [
        "# Training the model\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SupKMddf_wxs"
      },
      "source": [
        "Fitting the model with the data and finding out the accuracy at each epoch to see how our model is learning. Now I trained my model on 50 epochs and a batch size of 128. I can try using more number of epochs to increase accuracy but here I can see that the model has already reached a very high accuracy so I don't need to run it for more. During each epochs I can see how the model is performing by viewing the training and validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWigVc5D4vPu"
      },
      "source": [
        "# Saving model\n",
        "model.save(\"/content/drive/My Drive/bird_species.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U-Y-jpq_6sQ"
      },
      "source": [
        "Now I will save the model in h5 format to use it later for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IsevhAa5aWP"
      },
      "source": [
        "#Plot the training history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['accuracy'], color='r')\n",
        "plt.plot(history.history['val_accuracy'], color='b')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHgitDEbAGfL"
      },
      "source": [
        "Next I will plot the accuracy of the model for the training history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkLfcLZO5eHy"
      },
      "source": [
        "#Plot the loss history\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['loss'], color='r')\n",
        "plt.plot(history.history['val_loss'], color='b')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UopTb1aAQLR"
      },
      "source": [
        "Here we will plot the loss of the model for the training history."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uf_D53oO51E_"
      },
      "source": [
        "# Calculating test accuracy\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwmUAtItAbXM"
      },
      "source": [
        "Evaluating the model to know the accuracy of the model on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVjY4y0d57l3"
      },
      "source": [
        "# Storing predictions\n",
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmf5tWW0Agqn"
      },
      "source": [
        "Here we are storing prediction on test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvcqNQx559rh"
      },
      "source": [
        "# Plotting image to compare\n",
        "img = array_to_img(x_test[5])\n",
        "img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RSUASmOAltK"
      },
      "source": [
        "Visualizing one of the image which we will be further used for prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwTi2TXE5_lV"
      },
      "source": [
        "# Finding max value from predition list and comaparing original value vs predicted\n",
        "labels = lb.classes_\n",
        "print(labels)\n",
        "print(\"Originally : \",labels[np.argmax(y_test[5])])\n",
        "print(\"Predicted : \",labels[np.argmax(y_pred[5])])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuStfSn3AynN"
      },
      "source": [
        "Now, I created list of labels using object of label binarizer. I will print that list and finally I will print out the prediction and the original label of the image we visualized above using argmax(). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq05e6P_BesK"
      },
      "source": [
        "# Conclusion:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxED27zNBkD5"
      },
      "source": [
        "I started with loading the dataset into google colab using google drive and visualizing the images. Normalizing is an important step when working with any type of dataset. After that I created a CNN Model which is further used for predicting the bird species using the image supplied to model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kpokKOIBlyP"
      },
      "source": [
        "# Scope:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKp5b0DXBp6P"
      },
      "source": [
        "This project can be used for educational purposes to get a better understanding of how to create network architecture for a CNN model. We can further hyper parameter tune this model to reach a higher accuracy. It can be used by bird sanctuaries to identify different types of birds. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hg2F4xNNLije"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}